{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06086079-f838-4091-b724-30945e92f50e",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4b14-7797-4140-babe-857aa37a27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse #used to take values from command line interface\n",
    "from selenium import webdriver as wb  #used for websrapping using visually opening the browser\n",
    "import time       #handles the time data\n",
    "import os         #handles minor operating system functions\n",
    "import glob       # used for getting the files in order\n",
    "import shutil     # used for moving and renaming files properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fb71f-ed8f-4f89-aac6-58ccfe31cb38",
   "metadata": {},
   "source": [
    "### Function to scrape title and url data from google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdc39c-8af9-4ddb-85bb-33823da3e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_urls(query: str,thresh: int) -> list:\n",
    "\n",
    "    ''' query :: takes the word for the search \n",
    "        thresh :: is the number of top options to keep\n",
    "        returns a list of urls of top results'''\n",
    "\n",
    "    driver = wb.Chrome(\"./chromedriver.exe\")\n",
    "    # driver.minimize_window()  # Uncomment to minimize the window\n",
    "    driver.maximize_window()    # Comment it out to avoid the window maximizing\n",
    "    driver.get('https://scholar.google.com/')     #get function lets the browser driver to get to the url specified as the string\n",
    "    time.sleep(2)                                 #a little delay to let the elements load on the webpage\n",
    "    driver.find_element_by_xpath('//*[@id=\"gs_hdr_tsi\"]').send_keys(query)        #finding the textbox element to input the query \n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_xpath('//*[@id=\"gs_hdr_tsb\"]/span/span[1]').click()    #clicking the search button for the search \n",
    "    time.sleep(2) \n",
    "    main_elem = driver.find_elements_by_class_name('gs_rt')                       #finding the element that contains the titles and the corresponding urls\n",
    "    title_list= []                       #list that stores the titles\n",
    "    url_list=[]                          #list that stores the urls\n",
    "    for elem in main_elem:\n",
    "        title_list.append(elem.text)     #getting the text property of the element to get the title and appending the title to the title list\n",
    "        hr = elem.find_element_by_tag_name('a')      #getting the link associated with the element \n",
    "        url_list.append(hr.get_attribute('href'))    #now getting the href or url linked with the element \n",
    "    driver.close()                                   #closing the driver\n",
    "    try:\n",
    "        return title_list[:thresh],url_list[:thresh] #returning only the limited vlaues as per the requirement limit i.e. threshold value\n",
    "    except: \n",
    "        print(\"[--] Threshold value is either exceeding or falling behind\")\n",
    "        print(f\"[==] Length value of the list is {len(url_list)}\")\n",
    "        return title_list,url_list                  #if the threshold value is higher than the number of paper available\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b5763-16ed-4e3a-88a3-a6694823ca3e",
   "metadata": {},
   "source": [
    "### Function to download pdfs from sci-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cf117-6bf6-40a3-bde3-1697aea2cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sci_hub_download(url: str,down_time = 15) -> bool:\n",
    "    ''' url : string holding the web address\n",
    "        down_time : this integer holds the number of seconds the browser need to wait till the pdf download completes\n",
    "        returns either True or False if downloads thd pdf or not '''\n",
    "    driver = wb.Chrome()              #creating the driver\n",
    "    driver.maximize_window()          #maximizing the windows for visualization purposes \n",
    "    driver.get('https://sci-hub.mksa.top/')     #getting into the sci-hub website\n",
    "    driver.find_element_by_xpath('//*[@id=\"input\"]/form/input[2]').send_keys(url)    #finding the search bar and inputting the url into it \n",
    "    driver.find_element_by_xpath('//*[@id=\"open\"]').click()                          #clicking the button to search for article if available or not\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_xpath('//*[@id=\"buttons\"]/button').click()            #finding one way to get the save button that downloads the pdf \n",
    "        time.sleep(down_time)          #giving the browser some time to download the document \n",
    "        driver.close()\n",
    "        return True             #returning true if the download happends\n",
    "    except:\n",
    "        print('[-] COULD NOT FIND THE SAVE BUTTON')\n",
    "        print('[+] TRYING ANOTHER WAY ...')\n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            driver.find_element_by_xpath('//*[@id=\"buttons\"]/ul/li[2]/a').click()    #trying another way to get the save button that downloads the pdf\n",
    "            time.sleep(down_time)\n",
    "            driver.close()                                                   \n",
    "            return True\n",
    "        except:\n",
    "            print(\"[--] Could not find a way to download the pdf\")\n",
    "            return False                                #returning false if the download doesnot happengher than the length of the list then returns the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0617d0-73b8-4ba1-b8ce-3b048073db17",
   "metadata": {},
   "source": [
    "### Function to rename files properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ffb950-6910-4aa6-bad6-736c9bc6bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_rename(title: str ) -> str:\n",
    "    ''' Excludes the characters that are declined by the renaming policy of windows file system'''\n",
    "    excluder = [\":\",\"-\",\";\"]      # some values that is to be excluded \n",
    "    new_values = []\n",
    "    for value in list(title):\n",
    "        if not value in excluder and not value == \" \": new_values.append(value)      #deleting the excluder values excluding the space values\n",
    "        if value == \" \" : new_values.append(\"_\")                                     #replacing the space values with underscoer(_)\n",
    "    return \"\".join(str(item) for item in new_values)                                 #converting the new list into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421fb809-5e58-423d-bbef-41716908033a",
   "metadata": {},
   "source": [
    "### Function to place the files in proper directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b04294-3e13-4ee0-b427-9b39a94c9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_in_download(query : str ,title: str) -> None:\n",
    "    ''' query : this variable stores the search query and renames the results\n",
    "        title : this variable stores the title of the pdf downloaded for the renaming purposes '''\n",
    "    path = f\"C:/Users/avyar/Downloads/{query}\"                         #creating the path for directory for the query \n",
    "    if not os.path.exists(path[:-1]):                     #if the directory does not exists that this creats it\n",
    "        os.mkdir(path[:-1]) \n",
    "    list_o_files = filter(os.path.isfile, glob.glob(\"C:/Users/avyar/Downloads/\"+\"*\"))       #getting all the files in the folder \n",
    "    sorted_files = sorted(list_o_files, key=os.path.getmtime)                               #sorting the files according to recently modified\n",
    "    recent_one = sorted_files[-1]                                                           #getting the last files after sorting i.e. the file that is just downloaded\n",
    "    recent_one = recent_one.split(\"\\\\\")[-1]                                      #splitting the text using a \\ due to some unorthodox renaming convention created by the system\n",
    "    print(recent_one)\n",
    "    src_path = f\"C:/Users/avyar/Downloads/{recent_one}\"                           #mentioning the path of the new file\n",
    "    new_src_path = f\"C:/Users/avyar/Downloads/{refine_rename(title)}.pdf\"         #mentioning the path of the file after the modification\n",
    "    os.rename(src_path,new_src_path)                                              #renaming the file and it's location\n",
    "    dst_path = f\"C:/Users/avyar/Downloads/{query[:-1]}/{refine_rename(title)}.pdf\"    #defing the file path to move the file \n",
    "    shutil.move(new_src_path, dst_path)                                           #moving the file using some utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d9413-a8d6-40d9-a174-2b3144637e41",
   "metadata": {},
   "source": [
    "### Function to read queries from the local file \"queries.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb7c7d-1d55-42cd-944a-d089287380b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(path: str ) -> list:\n",
    "    ''' Reads multiple queries as per the text file mentioned in the path'''\n",
    "    with open(path, 'r') as f:queries = f.readlines()             # reading the file that contains the queries\n",
    "    return [query for query in queries if query[0] != \"#\"]        # returning the list of the queries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8fe90-7357-418a-a33b-10242116d89a",
   "metadata": {},
   "source": [
    "### Function to move all query directories to the \"./depostion\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84658edd-007c-47a8-835c-0ed65a08f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_final(queries : list):\n",
    "    ''' this function moves the files to the main directory of submission from the download folder'''\n",
    "    down_path = \"C:/Users/avyar/Downloads/\"              #specifying the download path as a string\n",
    "    depo_path = \"./deposition/\"                          #specifying the new directory path to move the pdf files \n",
    "    for query in queries:\n",
    "        shutil.move(down_path+f\"{query[:-1]}\",depo_path+f\"{query[:-1]}\")     #moving the files from download to the required location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf975a33-662d-4ca4-a775-2f52ec6eae51",
   "metadata": {},
   "source": [
    "### Defining main parameters and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073a366-de5f-4ced-b60c-766239d01c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description= \"give thershold value for the number of papers\")              #using argument parser for cli input\n",
    "    parser.add_argument(\"--thresh\",\n",
    "                        type = int,\n",
    "                        help = \"this param holds the threshold value of maximum value for the available papers\")    #creating args for the threshold numeric value\n",
    "    parser.add_argument(\"--down_time\",\n",
    "                        type = int,\n",
    "                        help = \"this param holds the threshold value of maximum value for the available papers\")    #creating ars for the download time delay\n",
    "    args = parser.parse_args()                                      #parsing the values from the cli\n",
    "    query_list = read_query(\"./queries.txt\")                       #reading the queries \n",
    "    thresh = args.thresh                                           #getting the threshold values from the parser\n",
    "    down_time = args.down_time                                     #getting the down_time values from the parser\n",
    "    for query in query_list:\n",
    "        title_list, url_list = get_title_urls(query,thresh)        #getting the titles and urls from the google scholar webpage\n",
    "        for title,url in zip(title_list,url_list):                 #iterating over the list to download the pdfs \n",
    "            if sci_hub_download(url,down_time):                    #if the download occures the file will be placed as per path specified\n",
    "                save_in_download(query,title)                    \n",
    "    move_final(query_list)                                         #finally moving the files from download to the required locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
